---
title: "Personal Activity Monitor Analysis"
author: "By Nithya"
output:  
  html_document:  
    keep_md: true 
---
&nbsp;

#Executive Summary

Now a days people are using several sensors to read how much (quantitatively) of a particular physical activity they are doing. Such sensor data is easily obtainable. But what this raw data lacks is a qualitative insight into how "well" an activity was performed. 

The analysis in this Document intends to achieve the above. We use a set of accelerometer readings on the belt, forearm, arm and dumbell obtained from 6 participants doing barebell lifting. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The outcome of accelerometers was then recorded and classified for their quality by assigning an ordinal variable with range A to E.

&nbsp;

#Approach

The Training Dataset ([Link to Training DS][1]) used has 19622 observations and 160 variables. The outcome (the one we need to predict) is stored in a variable named "classe". We would need to do the below with the datset - 

1. Divide it with a proportion of 70%:30% into Training and Validation Data sets.

2. Build the model on the Training data set containing 70% of actual number of observations.

3. Validate the model on Validation data set containing 30% of observations in order to cross-validate to find the out of sample error. 

Once we have concluded on the predictive model using the above steps, we would apply the same on the test dataset ([Link to Testing DS][2]) and predict the results.

&nbsp;

#Type of Model 

This is essentially a classification requirement. We need to classify or rank an activity-instance/-attempt on a scale of A to E via a concise and accurate model. We would essentially make use of descision trees which would take all the inputs and predict the results. 

**To benefit from the strength of bootstrapping to select a model for this classification requirement, we would use the Random Forest method.**

&nbsp;

#Data Retrieval, Cleansing and Preprocessing

```{r, message=FALSE}
library(knitr)
library(lubridate)
library(ggplot2)
library(gridExtra)
library(caret)
library(parallel)
library(doParallel)
library(randomForest)
```

&nbsp;

##Data Retieval
We would first load the datasets from the web urls containing the Training and Validation datasets.

```{r, cache=TRUE, message=FALSE}
download.file(url = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", destfile = "pml-training.csv")
act_data <- read.csv("pml-training.csv")

download.file(url = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", destfile = "pml-testing.csv")
testing <- read.csv("pml-testing.csv")
```

*Data Collection*: The abstract provided at the link - [Abstract][3] describes the data collection and classification Process (process of classifying the sensor readings of each record on a quality scale of A to E) in detail.

**About the Data**

1. Each row contains a distinct set of sensor readings taken at a given time instance.

2. We would be predciting the outcome stored in the variable named "classe"

3. There are variables for calculated features on the "Euler angles (roll, pitch and yaw)", "total acceleration", "gyros x/y/z", "acceleration x/y/z" and "magnet x/y/z" for each of the 4 sensors - "Belt", "Arm", "Forearm" and "Dumbell".

4. There are some timeseries variables in the begining that indicate the 2.5sec windows for collection of sensor readings.

5. There are a few apparently derived attributes which mostly are either NA or Blank. 

&nbsp;

##Data Cleansing

**1. User Names and Serial Numbers:**

We are not interested in how a person is performing. We only want to know how well a particular execution of activity is based on the sensor reading irrespective of any person. Therefore, we can drop the user name column from model selection process. Also we would drop Serial Number which does not convey any information.

```{r, message=FALSE}
final_data <- act_data[,c(-1,-2)]
testing_final <- testing[,c(-1,-2)]
```

**2. Time Series Data:**

The columns 3 to 7 are time series variables. This data pertains to the time windows for which the Sensor readings were obtained. We choose to exclude them from the prediction algorithm due to the below reasons - 

1. This is an incremental time series and we can not build a generic classification model (like random forest) by including the same. 

2. Eventhough via some other exploratory analysis on these datasets, it may appear that the timeseries data should be included (we may get slightly better prediction accuracy as well for these specific datasets), this may result in overfitting in general. 

3. The variables, new_window and num_window can be ignored as they are consolidation columns and not measured variables in true sense. 

4. Let us assume that there could be some influence of the hour of a day when the readings were taken (people may feel more active in the morning than evening or vice versa). We will now verify this assumption via the below graph where we have plotted the activity quality in cumulative %age against the hours.

```{r, message=FALSE}
final_data$cvtd_timestamp <- strptime(as.character(act_data$cvtd_timestamp),"%d/%m/%Y %H:%M")
testing_final$cvtd_timestamp <- strptime(as.character(testing$cvtd_timestamp),"%d/%m/%Y %H:%M")

hour <- hour(as.character(final_data$cvtd_timestamp))
time_data <- data.frame(hr = hour, classe = final_data$classe, count = 1)
time_agg <- aggregate(x = time_data[ ,3], by = list(as.vector(time_data$hr), as.vector(time_data$classe)), FUN = "sum")
names(time_agg) <- c("hr", "classe", "count")
for(i in 1:nrow(time_agg)) { time_agg[i, "total"] <- sum(time_agg[time_agg$hr == time_agg[i, "hr"], "count"]) }
time_agg$percentage <- ifelse(time_agg$total != 0, (time_agg$count *100 / time_agg$total), 0)

g1 <- ggplot(data = time_agg,aes(x=as.character(hr))) + geom_bar(aes(y = percentage, fill = classe), stat='identity') + xlab("Hour of a Day") + ylab("Percentage") + ggtitle("Activity Quality Composition vs Hour of a Day") 
print(g1)
```

There doesnt seem to be any obvious pattern that indicates the influence of the hour of a day on activity quality. 

Based on all of the above justifications, we choose to drop all the time series variables.

```{r, message=FALSE}
final_data <- act_data[,c(-1,-2,-3,-4,-5,-6,-7)]
testing_final <- testing[,c(-1,-2,-3,-4,-5,-6,-7)]
```

**3. NA and Blank Columns:**

As we can see, there are several rows with most/all of the values as NAs or Blanks. This could be due to various reasons investigating which is out of scope of our analysis now. These variables can not be considered for model selection. We would now filter out the variable which are NAs / Blanks for more than 75% of the rows.

```{r, message=FALSE}
indx1 <- ifelse(colSums(is.na(final_data) / nrow(final_data)) < 0.75, T, F)
final_data <- final_data[,indx1]
testing_final <- testing_final[,indx1]

indx2 <- ifelse(colSums((final_data == "") / nrow(final_data)) < 0.75, T, F)
final_data <- final_data[,indx2]
testing_final <- testing_final[,indx2]
```

After the above preprocessing and filtering, below are the **Final set of Columns we would base our prediction model on** - 

```{r, message=FALSE}
names(final_data)
```

&nbsp;

##Data Partitioning

```{r, message=FALSE}
set.seed(33433)
inTrain <- createDataPartition(final_data$classe, p = 0.7, list = FALSE)
training <- final_data[inTrain, ]
validation <- final_data[-inTrain, ]
```

&nbsp;

##Pre-Processing: Principal Component Analysis (PCA)
We would like to use Principal Component Analysis to find the components that capture majority of the variances in Training dataset. The below graph shows the outcome of PCA - 

1. Line: Cumulative percentages of variance (line) explained by the components

2. Bars: Variances explained by individual components. 

```{r, cache=TRUE, message=FALSE, fig.height=6, fig.width=10}
prcmp <- prcomp(training[,-ncol(training)], center = TRUE, scale. = TRUE)
pc <- data.frame(var <- (prcmp$sdev)^2)
names(pc) <- "Var"
pc$perc <- pc$Var * 100 / sum(pc$Var)
pc$cum <- cumsum(pc$Var)
pc$cum_perc <- cumsum(pc$perc)
pc$pc <- 1:nrow(pc)

g2 <- ggplot(data = pc) + geom_bar(aes(x = pc, y = perc, group = 1), stat = "identity", fill = "steelblue") + geom_point(aes(x = pc, y = cum_perc, group = 2), col = "red", size = 2) + geom_line(aes(x = pc, y = cum_perc, group = 2), col = "red") + xlab("Principal Components") + ylab("Variance Percentage")
print(g2)
```

As we can see, 90% of the variance can be explained by 20 components. While 95% of the variance can be explained by 25 components.

&nbsp;

#Model Selection

We would now fit a Random Forest Model on the training data set. The "train" function in R when called with "pca" as the method, would create random samples of the training dataset using bootstrapping. Again using bootstrapping, It would create random sets of variables to create a descision tree for each sample. At the end, all the individual trees would be merged / ensembeled and the predictions for each set of inputs would be averaged (majority vote) across the outcomes from these trees. Therefore, inspite of being a bit complicated, this model would give us the best accuracy.

We would also see if PCA provides any improvement in accuracy. 

This model selection process being very perfomance intensive, we would go for parellel processing

##With outcome from PCA
```{r, cache=TRUE}
training_pca <- predict(prcmp, newdata = training)
cluster <- makeCluster(detectCores() - 1) 
registerDoParallel(cluster)
fitControl <- trainControl(method = "cv", number = 10, allowParallel = TRUE)
Model_Fit_pca <- train(training$classe~., data = training_pca, method = "rf", trControl = fitControl)
```

**Model Accuracy With outcome from PCA**
```{r, message=FALSE}
results_pca <- Model_Fit_pca$results[order(-Model_Fit_pca$results[,2]), ]
kable(results_pca[1,], row.names = FALSE) 
```

##Without outcome from PCA
```{r, cache=TRUE}
Model_Fit <- train(classe~., data = training, method = "rf", trControl = fitControl) 
stopCluster(cluster) 
```

**Model Accuracy Without outcome from PCA**
```{r, message=FALSE}
results <- Model_Fit$results[order(-Model_Fit$results[,2]), ]
kable(results[1,], row.names = FALSE) 
```

Below are the results of prediction (in %ages) without PCA.
```{r, message=FALSE}
kable(confusionMatrix.train(Model_Fit)$table, align = "c", digits = 2)
```

**It is apparent from above that the model using PCA components only provides `r round(results_pca[1,2] * 100,2)`% accuracy. Comparitively the model that uses data without PCA, provides a better accuracy of `r round(results[1,2] * 100,2)`%. Also the best tree model selected uses `r results[1,1]` randomly selected variables for prediction (mtry field)**

&nbsp;

#Cross Validation

The R function "train" also perfoms a cross-validation of the prediction results using training dataset. Below are the results of the same without PCA. The Log of sample error has been plotted against the number of trees - 

```{r, message=FALSE, fig.height=6}
plot(Model_Fit$finalModel, log = "y", main = "Automatic Cross-Validation Results")
legend("topright", legend=unique(training$classe), col=unique(as.numeric(training$classe)), pch=19)
```

As we can see - 

1. The sample error decreases as more and more trees are ensembeled. 

2. The error rate becomes very flat (doesnt change much) after approx. 100 trees.

&nbsp;

Below we have plotted the overall Accuracy against the total count of randomly selected predictors. 

```{r, message=FALSE, fig.height=6}
plot(Model_Fit)
```

As we can see -

1. The accuracy remains between 99.15% to `r round(results[1,2] * 100,2)`% for the number of randomly selected predictors from 2 to `r results[1,1]`. 

2. The best accuracy if `r round(results[1,2] * 100,2)`% is achieved at the count of `r results[1,1]`

3. There is a sharp decline in accuracy with number of randomly selected predictors exceeding `r results[1,1]`

&nbsp;

##Cross-Validation using the validation dataset

Let us now see how well our models (with and without PCA) perform on the validation dataset.

###With outcome from PCA
```{r, message=FALSE}
validation_pca <- predict(prcmp, validation)
suppressMessages(cm1 <- confusionMatrix(validation$classe, predict(Model_Fit_pca, validation_pca)))
print(cm1$overall[1:4])
```

###Without outcome from PCA
```{r, message=FALSE}
suppressMessages(cm2 <- confusionMatrix(validation$classe, predict(Model_Fit, validation)))
print(cm2$overall[1:4])
kable(cm2$table, align = "c")
```

**As evident from above results, the PCA model provides `r round(cm1$overall[[1]] * 100,2)`% accuracy whereas the model that uses data without PCA, provides a better accuracy of `r round(cm2$overall[[1]] * 100,2)`%.**

**Looking at the above results, the Final Model selected would be the one without PCA.** Below are the 20 most important variables in the order of their influence with respect to the final model.

```{r, message=FALSE, fig.height=8}
plot(varImp(Model_Fit))
```

&nbsp;

##Out of Sample Error

**With the selected model, the out of sample error is : `r round((1 - cm2$overall[[1]]),4)` or `r round((1 - cm2$overall[[1]]) * 100,2)`%**

&nbsp;

#Applying the model on Testing Dataset

We would now go ahead and apply the model on the Testing Dataset with 20 observations with unique problem IDs available at - [Link to Testing DS][2]. 

**Below are the predictions for each of the problem IDs using the Random Forest model that we selected.**

```{r, message=FALSE}
testing_final <- testing_final[,1:(ncol(testing_final)-1)]
testing_final$classe <- predict(Model_Fit, testing_final)
testing_final$problem_id <- testing$problem_id
kable(testing_final[,c("problem_id","classe")], align = "c", col.names = c("Problem ID on Testing Set", "Predicted Outcome"), caption = "Results of Prediction Model for Testing dataset")
```


[1]: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
[2]: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv
[3]: http://groupware.les.inf.puc-rio.br/har